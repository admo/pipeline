[/
 / Boost.Pipeline documentation 
 /
 / Copyright 2014 Benedek Thaler
 /
 / Distributed under the Boost Software License, Version 1.0. (See accompanying
 / file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
 /]
 
[library Boost.Pipeline
  [quickbook 1.6]
  [version 1]
  [/ [authors [Thaler, Benedek]] ]
  [copyright 2014 Benedek Thaler]
  [category concurrent] 
  [id pipeline] 
  [dirname pipeline]
  [purpose
      Parallel execution of operations on batch data.
  ]
  [license
      Distributed under the Boost Software License, Version 1.0.
      (See accompanying file LICENSE_1_0.txt or copy at
      [@http://www.boost.org/LICENSE_1_0.txt])
  ]
]

[template fileref[path]'''<ulink url="https://github.com/erenon/pipeline/blob/master/'''[path]'''">'''[path]'''</ulink>''']

[def __queue__ [@http://www.boost.org/doc/libs/1_56_0_b1/doc/html/thread/sds.html#thread.sds.synchronized_queues `queue`]]

[section Introduction]

The goal of this library is to allow parallel execution of operations on batch data.
The design is based on the [@http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3534.html N3534] paper.
This work is founded by Google through the Google Summer of Code 2014 program.

[h1 Motivation]

The UNIX pipeline allows programs to build a multi threaded chain of transformations in an
easy a reliable way. The pipeline described by the following snippet is distributable across
processors, free of deadlocks, data races and undefined behavior:

    $ grep 'Error:' error.log | grep -v 'test-user@example.com' | sed 's/^User:.*Error: //' > output.txt

This reads a logfile, selects errors and filters events generated by test users
then feeds the message to the output file. Creating such a pipeline in C++ using synchronization
primitives, queues and threads is possible but not as intuitive as this.
This library intends to make describing such chains easy.

Aside simple pipelines, it's a common application design to separate input processing
into different stand-alone modules which communicate through message passing and executed
by a thread pool. For example, an HTTP server might act this way: There are threads reading
the requests from sockets, others process them, again others send them.

The current [link pipeline.scheduling scheduling] schemes used in the library do not support low latency
applications very well but it's planned to be changed to prevail on this area as well.

[important
  This is not an official Boost library and is under development.
  The interface is subject of change and currently it's not recommended to build production
  applications top of this library.
]

[endsect]

[section Example]

The following (incomplete) example shows a simple way to utilize the pipeline using [funcref boost::pipeline::from from()]:

[import ../example/hello.cpp]
[example_hello_body]

This will feed the contents of `input` to the pipeline which trims whitespace from each items,
selects those which starts with "Error", prepends them by an arrow mark them puts them into `output`.
Please find the whole program in the [fileref example/hello.cpp] file. This snippet is intended to give a
quick glimpse of the interface. The [link pipeline.tutorial Tutorial] section walks through building a pipeline.

[endsect]

[section Tutorial]

[import ../example/tutorial.cpp]

This section explains various parts of a pipeline and shows ways how to build them.
The example snippets below assume the following include and namespace alias:

[example_tutorial_preamble]

[h1 Input]

Every complete pipeline requires a source of input. Such source can be a container,
a range, a __queue__ or a generator. A container or a range is an input of a fixed size:
once it's assigned, it cannot (or shouldn't) be changed later. The __queue__ is specific
to this library; it provides concurrent read and write, therefore items can be added to it
even after the pipeline is launched. The generator is a function (1) or a callable object with (2):

    R generator(ppl::queue_back<T> downstream); // (1)
    R operator()(ppl::queue_back<T> downstream); // (2)

The generator function feeds the underlying queue through `queue_back` and returns when there
are no more items to be processed. All three types of input can be turned into the beginning
of a pipeline using `from()`:

    ppl::from(container) // takes begin and end
    ppl::from(container.begin(), container.end()) // same as above
    ppl::from(queue) // reference is taken
    ppl::from(generator) // generator gets copied

Please refer to the [link header.boost.pipeline.pipeline_hpp API documentation] to learn more about using `from()`.
Transformations can be connected later to the returned /segment/ to form a pipeline:

[h1 Connecting transformations]

Transformations are the meat of a pipeline. Input segments created by `from()` are not much use by
themselves. A transformation is a callable
(a function pointer, a `function` object, a bind expression, a functor or a lambda)
which receives one or more input items on each call and creates one or more output items in turn.
The signature of such transformations include:

    Output one_to_one(const Input& input);
    R      one_to_n  (const Input& input,           queue_back<Output>& downstream);
    Output n_to_one  (queue_front<Input>& upstream);
    R      n_to_m    (queue_front<Input>& upstream, queue_back<Output>& downstream);

Neither of the above cases can `Output` be `void`.
Please refer to the [link pipeline.transformations Transformations] section to read more about them.

Applying transformations on the input is done by the `|` operator. Each use of the
operator creates a new segment and therefore it's chainable:

[example_tutorial_chaining]

See [fileref example/tutorial.cpp] for more.
After all the required transformations are connected, to output must be described:

[h1 Specifying pipeline output]

The pipeline is incomplete until the output is specified. The output can be a container,
a __queue__, or a consumer. A consumer is a callable receiving a `queue_front` which can be pulled
for items until there is no more. Usually, the library can recognize when an output segment is
specified, however, there is no difference between consumers and n-to-one transformations.
For such cases, use [funcref boost::pipeline::to to()]:

    void consumer_a(queue_front<int> upstream); // returns void
    int  consumer_b(queue_front<int> upstream); // returns int, might be mistaken for an n-to-one transformation

    segment | container; // container is recognized as output, taken by reference
    segment | queue;     // same as above
    segment | consumer_a // definitely a consumer, gets copied
    segment | ppl::to(consumer_b) // `to()` must be used to be recognized as consumer

Now our pipeline is assembled and ready to run:

[h1 Running the pipeline]

Until now we only have our pipeline assembled but not ran. To run a pipeline, its associated
`run()` method must be called with a [classref boost::pipeline::thread_pool thread pool].
This method schedules the transformations on the pool
and returns an [classref boost::pipeline::execution execution] object.
This handle can be queried or waited on:

[example_tutorial_run]

Please take a look at the [link pipeline.scheduling Scheduling] section to learn how to size
a thread pool to avoid deadlocks.

[endsect]

[section Glossary]

This section provides definitions of some commonly used terms throughout the documentation
or library code:

* *callable*: A function pointer, function object, functor, bind expression or lambda.
      Basically, every `f` is a callable if `f(Args...)` is valid for some `Args...`.
* *transformation*: A method producing output items from input items.
      Precisely, it's a callable which signature matches to one of them
      described in section [link pipeline.transformations Transformations]
* *segment*: A not necessarily terminated series of connected transformations.
* *plan*: A terminated series of connected transformations: its input and output is specified.
* *task*: A running segment. Tasks are scheduled on the thread pool.
* *upstream*: Generally, the segment connected to the left of a segment.
      Specifically, it's the input queue of a task.
* *downstream*: Generally, the segment connected to the right of a segment.
      Specifically, it's the output queue of a task.

[endsect]

[section Transformations]

A transformation is a callable which receives one or more input items and
produces one or more output items in turn. Depending on exactly how much items
are read and produced on each call, transformations have different signatures:

    // not exhaustive list of possible signatures
    Output one_to_one(const Input& input);
    R      one_to_n  (const Input& input,           queue_back<Output>& downstream);
    Output n_to_one  (queue_front<Input>& upstream);
    R      n_to_m    (queue_front<Input>& upstream, queue_back<Output>& downstream);

Type legend:

* *Input*: type of input items
* *Output*: type of output items
* *R*: arbitrary return type

Input items can be taken as `const&` (as above) or by value (if it's possible). Queue handles
can be taken by value or by reference; a `const&` doesn't make much sense.

A transformation can be anything which is callable with any of the above arguments.
This includes function pointers, function objects, functors, bind expressions and lambdas.
Examples:

    std::size_t length(const std::string& input);
    std::function<std::size_t(const std::string&)> length_f(length);
    auto inverse = [] (int input) { return input * (-1); };
    auto add_5 = std::bind(add, 5, _1);

Such transformations can be connected to appropriate segments using the `|` operator.
However, there are one restriction on bind expressions. Assume the following example:

    // takes the square root of input if it's greater than `threshold`
    void sqrt_if_greater(int threshold, queue_front<int>& upstream, queue_back<float>& downstream);

    auto sqrt_if_greater_than_5 = std::bind(sqrt_if_greater, 5, _1, _2);

This case, the library has no way to find out the valid argument list of the `operator()` template
the bind has; it can't guess the `value_type` of the downstream queue (`float`).
To overcome this limitation, the application must provide a hint:

[important
  If a callable is to be form an n-to-m transformation using bind
  and the input and output types are different,
  the type of the return value of the callable must match the output type.
]

Which means, in the example above, `sqrt_if_greater` must return `float`.
If the input and output types are the same, the library can get away with it.

You might wonder how an actual transformation looks like;
take a look at the [fileref example/transformations.cpp] file.

[endsect]

[section Open Segments]

[import ../example/open_segment.cpp]

Sometimes it's not feasible to assemble a pipeline in one go. One might want
to build up smaller chunks and connect them dynamically. It's possible to create
incomplete plans (i.e: /segments/) using [funcref boost::pipeline::make make()]:

[example_open_segment_auto]

Segments created by `make`, such as `s2` in the example are non-terminated segments
([fileref include/boost/pipeline/detail/open_segment.hpp]) and not much of use on their own.
To run them, they must be connected to an appropriate right-terminated but left-open segment (e.g: `s1`).

[h1 Type erasure]

It might be necessary to take segments or plans as arguments, therefore a type is required.
We always used the `auto` keyword so far when it came to segments, because the actual type
is rather complex and internal to the library. However, type erased handles are provided
to be able to refer to them. [classref boost::pipeline::segment segment<Input, Ouput>] can
refer to any segment or plan which takes `Input`s as input and produces `Output`s. The tag
`terminated` denotes a closed end. A complete pipeline can be referenced as
`segment<terminated, terminated>` or just simply `ppl::plan`:

[example_open_segment_type_erasure]

Thanks to the type erased handles, it's easy to create interfaces expecting
segments or plans:

[example_open_segment_type_erasure_interface]

However, using these aliases does not come for free: it involves allocation of dynamic memory
and indirection. It's recommended to use the `auto` keyword instead wherever it's possible.

[endsect]

[section Scheduling]

The point of creating pipelines is to turn concurrency into parallelism
by breaking a large process into smaller independent tasks
and connecting them through message passing. Task can be then executed
by different threads in parallel, thus achieving better throughput and
— if done well — even lower latency.

Unfortunately, the scheduling mechanism currently employed by the library is
far from great or clever. Below, the pros and cons of this solution is described
before the planned improvements.

[h1 Scheduling as done currently]

It's challenging to schedule tasks because using standard tools only,
the library is not able to preempt them, therefore they might block for a long time.
Also, even with the application writer involved, it's not convenient yielding from a task
to an other in the same thread, e.g: if the downstream queue is full.

Such concerns can be resolved by using coroutines, but coroutine support is not yet implemented.
Until then, the library uses an unbounded synchronized __queue__ to connect segments, a top-down
approach to schedule tasks and employs blocking instead of yielding.

This is how it works: When the queue is being run, the tasks representing the plan
are submitted to the thread pool, starting at the beginning of the pipeline.
Order is important, this way the pool will execute the first `pool_size` segment.
Because of the reasons above, execution of a segment will not stop until the input
queue is closed. If the input queue is empty, it will block until new item is available
or it gets closed; this way no precious cycles are wasted spinning. The queue is unbounded,
which means there is no blocking because of a full output queue.
Assuming a pipeline of length `segment_count`, if `segment_count > pool_size`, the `pool_size + 1`th
segment will run only if the first segment is finished.  This might affect latency badly.
To avoid this, it's recommended to make the pool at least as large as the pipeline is long.

[h1 Planned improvements of scheduling]

It's clear the scheme used above is not optimal. The offending reentrancy constraints
can be mitigated by introducing coroutines. Running each task in it's own coroutine
would make yielding possible: every time the upstream queue is empty or the downstream queue
is full even after some spinning (assuming bounded lockfree queues).

Please refer to the [@http://www.slideshare.net/erenon/boostpipeline-scheduling-of-segments-36832455 Pipeline.Scheduling] 
slides for more information.

[endsect]

[section Cookbook]

Although the library is in a preview state and the interface is expected to change, here are some
constructs which implement common operations.

[h1 Split operation]

[import ../example/split.cpp]

It's a common task to dispatch an upstream queue to different downstream queues.
The following example provides an example of how to decrease latency of priority
requests by processing them in a separate pipeline:

[example_split_splitter]
[example_split_invocation]

Please refer to [fileref example/split.cpp] for the full source code.

[h1 Join operation]

[import ../example/join.cpp]

One might want to combine two different upstream queues into a single one,
like the unix `join` command does. The following example matches persons
to associated departments:

[example_join_joiner]
[example_join_invocation]

Please refer to [fileref example/split.cpp] for the full source code.

[endsect]

[section Build and Test this library]

This library requires a compliant C++11 compiler. It's tested using GCC 4.8.1
and clang 3.4. The msvc shipped with Visual Studio 2013 fails to compile the tests.

The library is header only, nothing specific has to be compiled to use it, however,
it depends on Boost.Thread, which has some libraries to link.

The tests, examples and documentation can be compiled using the following commands:

    # build and run tests
    test/$ BOOST_ROOT=/path/to/boost/ bjam toolset=gcc # or clang

    # build examples
    build/$ BOOST_ROOT=/path/to/boost/ bjam toolset=gcc # or clang

    # build documentation: requires docbook-xsl
    doc/$ BOOST_ROOT=/path/to/boost/ bjam

[endsect]

[section Acknowledgments]

This work is founded by Google through the Google Summer of Code 2014 program.

I'd like to thank my GSoC'14 mentor, Vicente J. Botet Escriba for providing guidance
and regular reviews. Also thanks for the Boost members participating in the discussion
regarding the scheduling on the Boost mailing list.

[endsect]

[section What's next]

TODO describe each task, affected module, severity and difficulty

* thread pool type erasure and documentation
* queue_back/queue_front type erasure
* parallel()
* inefficient queue_output_task
* test all combination of segments
* remove invalid methods from segment<I,O> interface
* make from(const container) work
* use coroutine/fibers

[endsect]

[xinclude apidoc.xml]
